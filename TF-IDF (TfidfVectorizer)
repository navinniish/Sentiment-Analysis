
#Import the necessary libraries 

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

#Mention the text that you would like to use. You can also use a dataframe
text = "I like this cat"

# Tokenize the text
tokens = word_tokenize(text)

# Remove stopwords
stop_words = set(stopwords.words('english'))
filtered_tokens = [token for token in tokens if token.lower() not in stop_words]

# Initialize CountVectorizer
count_vectorizer = CountVectorizer()

# Fit and transform the text data
count_matrix = count_vectorizer.fit_transform(filtered_tokens)

# Convert the matrix to an array
feature_names = count_vectorizer.get_feature_names()
count_features = count_matrix.toarray()

# Print the feature names and counts
print("Feature Names:", feature_names)
print("Count Features:", count_features)

# Initialize TfidfVectorizer
tfidf_vectorizer = TfidfVectorizer()

# Fit and transform the text data
tfidf_matrix = tfidf_vectorizer.fit_transform(filtered_tokens)

# Convert the matrix to an array
feature_names = tfidf_vectorizer.get_feature_names()
tfidf_features = tfidf_matrix.toarray()

# Print the feature names and TF-IDF values
print("Feature Names:", feature_names)
print("TF-IDF Features:", tfidf_features)


